"""
æ–‡æ¡£çˆ¬å–å·¥å…·
é›†æˆç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿï¼Œè·Ÿè¸ªåŸæ–‡æ—¶é—´ã€å˜æ›´æ—¶é—´å’Œæ±‰åŒ–çŠ¶æ€
"""

import os
import requests
import xml.etree.ElementTree as ET
from urllib.parse import urlparse
from datetime import datetime, timezone
from version_control import VersionControl

SITEMAP_URL = "https://mythicprefixes.superiormc.cn/sitemap-pages.xml"
BASE_URL = "https://mythicprefixes.superiormc.cn"

# è·å–é¡¹ç›®æ ¹ç›®å½• (scripts çš„ä¸Šçº§ç›®å½•)
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
OUTPUT_DIR = os.path.join(PROJECT_ROOT, "docs")

def fetch_sitemap(url):
    print(f"æ­£åœ¨è·å– sitemap: {url}...")
    response = requests.get(url)
    response.raise_for_status()
    return response.content

def parse_sitemap(xml_content):
    root = ET.fromstring(xml_content)
    namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
    items = []
    for url in root.findall('ns:url', namespace):
        loc = url.find('ns:loc', namespace).text
        lastmod_elem = url.find('ns:lastmod', namespace)
        lastmod = lastmod_elem.text if lastmod_elem is not None else None
        items.append({'loc': loc, 'lastmod': lastmod})
    return items

def parse_date(date_str):
    if not date_str:
        return None
    try:
        if date_str.endswith('Z'):
            date_str = date_str[:-1] + '+00:00'
        return datetime.fromisoformat(date_str)
    except ValueError:
        return None

def download_file(url, local_path, vc: VersionControl, lastmod: str = None):
    """
    ä¸‹è½½æ–‡ä»¶å¹¶æ³¨å†Œåˆ°ç‰ˆæœ¬æ§åˆ¶
    """
    print(f"  æ­£åœ¨ä¸‹è½½: {url}")
    try:
        response = requests.get(url)
        if response.status_code == 200:
            content = response.content
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–
            rel_path = os.path.relpath(local_path, PROJECT_ROOT)
            existing_info = vc.get_file_info(rel_path)
            
            if existing_info:
                current_hash = vc.compute_hash(content)
                if existing_info["original_hash"] == current_hash:
                    print(f"    [è·³è¿‡] å†…å®¹æ— å˜åŒ–")
                    return False
            
            # ä¿å­˜æ–‡ä»¶
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            with open(local_path, 'wb') as f:
                f.write(content)
            
            # æ³¨å†Œåˆ°ç‰ˆæœ¬æ§åˆ¶
            vc.register_original(rel_path, content, lastmod)
            return True
        else:
            print(f"    [å¤±è´¥] çŠ¶æ€ç  {response.status_code}")
            return False
    except Exception as e:
        print(f"    [é”™è¯¯] {e}")
        return False

def main():
    print("="*60)
    print("æ–‡æ¡£åŒæ­¥å·¥å…·")
    print("="*60 + "\n")
    
    # åˆå§‹åŒ–ç‰ˆæœ¬æ§åˆ¶ (ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•)
    vc = VersionControl(PROJECT_ROOT)
    
    try:
        xml_content = fetch_sitemap(SITEMAP_URL)
        items = parse_sitemap(xml_content)
        
        print(f"\nğŸ“„ Sitemap ä¸­å‘ç° {len(items)} ä¸ªé¡µé¢\n")
        print("-"*60)
        
        new_count = 0
        updated_count = 0
        skipped_count = 0
        
        # è®°å½•æœ¬æ¬¡å¤„ç†çš„æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæ£€æµ‹åˆ é™¤
        processed_files = set()

        for item in items:
            page_url = item['loc']
            lastmod = item['lastmod']
            
            # ç¡®å®šç›®æ ‡ URL å’Œæœ¬åœ°è·¯å¾„
            if page_url.rstrip('/') == BASE_URL.rstrip('/'):
                target_url = f"{BASE_URL}/welcome.md"
                local_path = os.path.join(OUTPUT_DIR, "welcome.md")
            else:
                if page_url.endswith('/'):
                    page_url = page_url[:-1]
                
                target_url = f"{page_url}.md"
                rel_path = page_url.replace(BASE_URL, '').lstrip('/')
                local_path = os.path.join(OUTPUT_DIR, f"{rel_path}.md")
            
            rel_path = os.path.relpath(local_path, PROJECT_ROOT)
            processed_files.add(rel_path)
            
            existing_info = vc.get_file_info(rel_path)
            
            if existing_info is None:
                # æ–°æ–‡ä»¶
                print(f"ğŸ†• æ–°æ–‡ä»¶: {rel_path}")
                if download_file(target_url, local_path, vc, lastmod):
                    new_count += 1
            else:
                # å¦‚æœæ–‡ä»¶ä¹‹å‰è¢«æ ‡è®°ä¸ºåˆ é™¤ï¼Œç°åœ¨åˆå‡ºç°äº†ï¼Œæ¢å¤å®ƒ
                if existing_info.get("status") == "deleted":
                    vc.restore_file(rel_path)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆåŸºäº lastmodï¼‰
                remote_dt = parse_date(lastmod)
                original_dt = parse_date(existing_info.get("original_modified"))
                
                should_check = False
                if remote_dt and original_dt:
                    if remote_dt > original_dt:
                        should_check = True
                        print(f"âš¡ æ£€æµ‹åˆ°æ›´æ–° ({lastmod}): {rel_path}")
                elif lastmod:
                    # æœ‰æ–°çš„ lastmod ä½†æ²¡æœ‰è®°å½•ï¼Œæ£€æŸ¥å†…å®¹
                    should_check = True
                
                if should_check:
                    if download_file(target_url, local_path, vc, lastmod):
                        updated_count += 1
                    else:
                        skipped_count += 1
                else:
                    print(f"â­ï¸  è·³è¿‡ (æ— æ›´æ–°): {rel_path}")
                    skipped_count += 1
        
        # æ£€æµ‹å·²åˆ é™¤çš„æ–‡ä»¶
        all_tracked_files = set(vc.metadata["files"].keys())
        deleted_files = all_tracked_files - processed_files
        
        if deleted_files:
            print("\n" + "-"*60)
            print("ğŸ—‘ï¸  æ£€æµ‹åˆ°ä»¥ä¸‹æ–‡ä»¶å·²ä» Sitemap ä¸­ç§»é™¤:")
            for deleted_path in deleted_files:
                # å¿½ç•¥å·²ç»è¢«æ ‡è®°ä¸ºåˆ é™¤çš„æ–‡ä»¶
                if vc.get_file_info(deleted_path).get("status") != "deleted":
                    print(f"   - {deleted_path}")
                    vc.mark_file_deleted(deleted_path)

        print("\n" + "-"*60)
        print(f"\nğŸ“Š åŒæ­¥å®Œæˆ:")
        print(f"   ğŸ†• æ–°å¢: {new_count}")
        print(f"   ğŸ”„ æ›´æ–°: {updated_count}")
        print(f"   â­ï¸  è·³è¿‡: {skipped_count}")
        if deleted_files:
            print(f"   ğŸ—‘ï¸  åˆ é™¤: {len(deleted_files)}")
        
        # æ‰“å°ç‰ˆæœ¬æ§åˆ¶æ‘˜è¦
        vc.print_summary()
            
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
